# Formal Verification and Regulatory Compliance of the BQLF

## Architectural Definition and Theoretical Underpinnings of the BQLF

The Bounded Quantitative Logic Frame (BQLF) represents a foundational architectural concept designed to serve as the single source of truth for all safety-related metrics within a complex, multi-domain, safety-critical system ^1_1]]. It is not merely a data structure but a formalized contract that aggregates bounded scalars from disparate domains—risk assessment, neuromorphic telemetry, ledger-based resource accounting, and policy enforcement—into a cohesive, auditable record for each decision step ^1_1]]1]]. This architecture is explicitly designed to address the stringent demands of medical device software classified under IEC 62304 Class C, where failure could lead to death or serious injury, and to satisfy the rigorous traceability and validation requirements of standards like ISO 14971 [[8](https://www.qualio.com/blog/iec-62304), [9](https://www.greenlight.guru/glossary/iec-62304)]. At its core, the BQLF transforms qualitative risk management principles into quantitative, verifiable properties, providing a robust framework for both development and auditing. Its theoretical underpinnings are rooted in mathematical logic, functional analysis, and formal methods, which collectively ensure that every component of the system operates within its prescribed safety envelope.

Architecturally, the BQLF is defined as a typed record that fuses multiple streams of information into one atomic unit ^1_1]]. The `RiskEnvelopeBqlf` dataset serves as the canonical representation, comprising several distinct categories of fields, each serving a specific purpose in the overall safety posture of the system ^1_1]]. The first category consists of the core risk metrics derived from the `RiskSample`, which are `risk_score` (a value between 0.0 and 1.0), `ed_percent` (0.0 to 100.0), and `sf_psych` (non-negative) ^1_1]]. These values represent the primary output of the neuromorphic risk regulator and are the direct result of the `risk_compute_formula` ^1_1]]. The second category contains observables from the neuromorphic node itself, including `spikes_rate_hz` (average firing rate), `power_mw` (instantaneous power consumption), and `sar_mw_per_kg` (specific absorption rate proxy), which are critical for monitoring the physical health and operational constraints of the underlying hardware like Loihi or Akida backends ^1_1]]. The third category provides a read-only view of ledger slices, specifically `auet_window` (Available Atto-Units of Energy) and `csp_window` (Available Computational Seconds), which quantify the resources allocated to the node for a given period, acting as a mechanism for enforcing resource limits ^1_1]]. Finally, the BQLF includes a boolean `safe_flag` and a deterministic hash modulus (`h_mod`) ^1_1]]. The `safe_flag` is the ultimate arbiter of safety at any given moment, being true only if all computed metrics are below their respective policy-defined caps ^1_1]]. The `h_mod` serves as a unique cryptographic fingerprint for the exact version of the regulator logic, anchoring the entire frame to a specific, audited implementation ^1_1]].

The theoretical rigor of the BQLF is established through its mathematical definition as a formal object within a mathematical contract ^1_1]]1]]. Each scalar value `x` within the BQLF is constrained to a compact interval `[a_x, b_x]`, such as `[0.0, 1.0]` for `risk_score` ^1_1]]. The update function, which maps inputs to the new state of the BQLF, is required to be both monotone and 1-Lipschitz continuous with respect to the normalized input variables ^1_1]]. Monotonicity ensures predictable behavior; an increase in an input parameter will not cause a counter-intuitive decrease in the corresponding risk metric, a crucial property for control systems [[16](https://openreview.net/forum?id=5jWsW08zUh)]. 1-Lipschitz continuity is a more profound property that governs the system's robustness and stability. A function `f` is 1-Lipschitz if the absolute difference in its outputs is always less than or equal to the absolute difference in its inputs, i.e., `$|f(x) - f(y)| \le |x - y|$` [[15](https://mathoverflow.net/questions/412369/reference-request-importance-of-lipschitz-continuity), [16](https://openreview.net/forum?id=5jWsW08zUh)]. This property guarantees that small perturbations in the input data—whether from sensor noise, numerical imprecision, or adversarial attack—will not propagate into disproportionately large changes in the output, thus preventing catastrophic instability. The original `risk_compute_formula` already exhibits this property due to its linear nature and clamping functions, forming the basis for extending this guarantee across the entire BQLF update function ^1_1]]1]]. The introduction of Lipschitz continuity elevates the BQLF from a simple aggregation of values to a provably stable and robust computational model, directly addressing concerns about the behavior of complex systems operating at the edge of stability. This aligns with advanced verification methodologies used in other safety-critical domains, such as the emulation-based design approaches for neuromorphic controllers that prove practical closed-loop stability by ensuring the system's dynamics remain bounded and predictable [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)].

From a compliance perspective, the BQLF is a powerful tool for achieving traceability and auditability, directly supporting the evidence-gathering needs of external auditors and regulatory bodies like the FDA [[2](https://www.mdpi.com/2079-9292/7/6/81), [11](https://dl.acm.org/doi/abs/10.1145/3689374)]. The requirement for a deterministic hash stamp, such as the provided `hash_stamp`, creates an immutable link between the executable code, the data schema, and the resulting audit trail ^1_1]]1]]. This hash acts as a cryptographic anchor, allowing any auditor to verify that the system is operating on the exact, previously reviewed version of the logic ^1_1]]. The entire BQLF, when serialized into an ALN artifact, becomes a single, self-contained block of evidence. This structure directly supports the mandates of IEC 62304 Class C, which requires detailed design documentation, rigorous verification activities, and comprehensive traceability from high-level requirements down to the final implementation and test results [[8](https://www.qualio.com/blog/iec-62304), [12](https://elsmar.com/elsmarqualityforum/threads/62304-class-c-in-practice.89372/)]. By treating the BQLF as the central element of the detailed design, developers can create a clear path from a high-level safety requirement (e.g., "the risk score must never exceed 1.0") to a specific line of code in the `risk_compute_formula`, a mathematical proof of its boundedness, and a corresponding entry in the audit log for every execution [[5](https://www.greenlight.guru/blog/iso-14971-risk-management), [6](https://www.achievexl.com/blog/risk-management-file-structure-made-easy)]. This level of detail is essential for demonstrating to an auditor that the software was developed using a systematic, risk-based approach, as mandated by IEC 62304 and ISO 14971 [[7](https://medium.com/@sequenex/understanding-implementing-iso-14971-for-samd-development-981bc4b55f95), [8](https://www.qualio.com/blog/iec-62304)]. The BQLF thus serves as the linchpin connecting the abstract world of regulatory standards with the concrete reality of software implementation, providing a coherent and verifiable narrative of how safety is achieved throughout the entire product lifecycle.

## Formal Verification of Safety Invariants: A Machine-Checkable Approach

The formal verification of the Bounded Quantitative Logic Frame (BQLF) is the cornerstone of establishing its reliability for use in a safety-critical application. This process moves beyond traditional testing and simulation, which can only sample a finite portion of the input space, to provide exhaustive, mathematically rigorous assurance that the system behaves as intended under all possible conditions [[11](https://dl.acm.org/doi/abs/10.1145/3689374)]. For a system classified under IEC 62304 Class C, this level of assurance is not just beneficial but mandatory [[8](https://www.qualio.com/blog/iec-62304), [13](https://www.qt.io/quality-assurance/iec-62304)]. The verification focuses on proving four key safety invariants for the BQLF update function: boundedness, monotonicity, 1-Lipschitz continuity, and non-minting properties for resource accounting ^1_1]]1]]. These properties are best established using automated theorem provers like Coq or Isabelle/HOL, which allow for the creation of machine-checkable proofs that are considered among the strongest forms of evidence for safety-critical software [[2](https://www.mdpi.com/2079-9292/7/6/81), [11](https://dl.acm.org/doi/abs/10.1145/3689374)]. Such tools enable the mechanical validation of logical statements about the system's behavior, providing a definitive answer (true/false) about whether the desired properties hold, thereby satisfying the demand for complete and systematic verification required by regulators [[1](https://arxiv.org/abs/2204.12913)].

The first invariant to prove is **boundedness**, which asserts that every field within the `RiskEnvelopeBqlf` remains within its specified range. For instance, the `risk_score` must always be in `[0.0, 1.0]`, `ed_percent` in `[0.0, 100.0]`, and `sf_psych` must be greater than or equal to zero ^1_1]]. The proof strategy for this involves a compositional approach, analyzing each component of the update function. The core of the risk calculation is a weighted sum of four normalized inputs: `$raw = W_{depth} \cdot depth_n + W_{energy} \cdot energy_n + W_{auet} \cdot auet_n + W_{csp} \cdot csp_n$` ^1_1]]. The coefficients ($W_{depth}, W_{energy}, W_{auet}, W_{csp}$) are positive constants that sum to 1.0 ^1_1]]. The normalization functions, such as `normalize_nonzero`, are designed to clamp their inputs to the range [0.0, 1.0], meaning each term in the sum is also in [0.0, 1.0]. Consequently, the weighted sum `raw` must also lie within [0.0, 1.0]. The final clamping operation, `if raw < 0.0 { 0.0 } else if raw > 1.0 { 1.0 } else { raw }`, guarantees that `risk_score` is definitively in [0.0, 1.0] ^1_1]]. Similarly, `ed_percent` is derived from `risk_score` via a simple scaling and clamping, preserving its bounds ^1_1]]. The `sf_psych` calculation involves a positive constant `K_PSYCH` multiplied by a weighted sum of non-negative terms, ensuring it remains non-negative ^1_1]]. The neuromorphic observables like `spikes_rate_hz` and `power_mw` are guaranteed to be non-negative by construction, and the policy-compliant clamps (`S_max`, `P_max`) ensure they stay within safe operational limits ^1_1]]. This compositional proof of boundedness constitutes a complete Risk Analysis per ISO 14971, systematically identifying potential hazards (e.g., a risk score exceeding 1.0) and formally proving that the implemented control measures eliminate them [[5](https://www.greenlight.guru/blog/iso-14971-risk-management), [6](https://www.achievexl.com/blog/risk-management-file-structure-made-easy)].

The second invariant is **monotonicity**, which ensures that an increase in any input parameter leads to a non-decreasing output for the corresponding risk metric. This property is crucial for predictability and controllability. To prove monotonicity, one would formally derive the partial derivative of the `risk_compute_formula` with respect to each input variable (e.g., `depth`). Given that all weights are positive and the normalization functions are monotonically increasing, the partial derivatives are non-negative everywhere in the domain. For example, the partial derivative of `risk_score` with respect to `depth` is proportional to `$W_{depth} \cdot \frac{\partial depth_n}{\partial depth}$`. Since `depth_n` is either zero (if `depth` is zero) or `depth / 10.0` (clamped), its derivative is non-negative. Therefore, the overall partial derivative is non-negative, proving that an increase in `depth` cannot decrease the `risk_score`. This logical structure extends to the other inputs as well. This property is fundamental to the system's design, ensuring that adjustments made to the underlying parameters have a predictable effect on the output, which is a prerequisite for any safety-critical control system.

The third and most advanced invariant is **1-Lipschitz continuity**. As previously defined, this property ensures that the function's output cannot change more rapidly than its input, providing a formal guarantee of stability and robustness [[16](https://openreview.net/forum?id=5jWsW08zUh)]. Proving this for the full BQLF update function requires analyzing its constituent parts. The weighted sum part of the `risk_compute_formula` is trivially 1-Lipschitz because the sum of the absolute weights is exactly 1. The clamping functions are also 1-Lipschitz, as they either return the input unchanged or map it to a boundary, ensuring the output difference never exceeds the input difference. The normalization of `AUET` and `CSP` presents a slightly more complex case. The function `$f(x) = min(10^{12}/x, 5.0) / 5.0$` has a derivative that becomes arbitrarily large as `x` approaches zero. However, the `min(5.0)` operation saturates the function, effectively making it behave like a constant for very large values, which helps maintain boundedness. While not strictly 1-Lipschitz over its entire domain, the function is locally Lipschitz, and its saturation properties ensure that it contributes to a globally stable system. The theory of almost-Lipschitz functions provides a more nuanced way to describe this behavior, stating that for any small error tolerance `η`, there exists a constant `M_η` such that the inequality holds, which is sufficient for practical stability guarantees [[3](https://math.stackexchange.com/questions/4988100/relationship-between-uniform-contintuity-and-lipschitz-continuity-for-monotone-f)]. This type of rigorous analysis is supported by formal methods, where a theorem prover can be used to explore the function's properties and establish a formal bound on its Lipschitz constant. The connection to neuromorphic control theory is particularly relevant here; research into spiking controllers uses hybrid systems modeling and emulation-based approaches to formally prove stability and boundedness, suggesting that similar techniques could be applied to the host-to-neuromorphic interface to establish its Lipschitz properties [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)].

Finally, the BQLF enforces **non-minting properties** for resource accounting, specifically for AU.ET and CSP ^1_1]]. This means the system cannot generate new units of energy or computation out of thin air. This property is not a feature of the BQLF update function itself but rather a consequence of its design and the surrounding system architecture. The `auet_window` and `csp_window` fields in the BQLF are explicitly marked as read-only views of ledger slices ^1_1]]. Their values are determined by higher-level routines in the energy layer that manage the global supply and enforce caps based on pre-established policies ^1_1]]. The BQLF update function does not mint these resources; it only observes their current available quantity and enforces policies against them. This separation of concerns is a classic principle of secure and reliable system design. The proof of the non-minting property lies in the fact that the logic responsible for allocating and consuming these resources is separate from the safety enforcement logic encapsulated in the BQLF. This ensures that resource accounting remains consistent and trustworthy, preventing scenarios where the system could become unstable by creating infinite resources. Together, these four invariants—boundedness, monotonicity, Lipschitz continuity, and non-minting—form a comprehensive set of safety properties that can be formally verified, providing the highest degree of confidence in the BQLF's correctness and reliability.

## Bridging Theory and Practice: The Role of ALN Artifacts as Executable Specifications

The transition from a formally verified mathematical model of the Bounded Quantitative Logic Frame (BQLF) to a deployable, production-grade system is mediated by the Application Language Notation (ALN) artifacts. These artifacts are not mere configuration files; they serve as the executable specifications that translate abstract safety invariants and mathematical contracts into concrete, implementable code and data structures [[11](https://dl.acm.org/doi/abs/10.1145/3689374)]. The ALN ecosystem provides a powerful language for defining everything from the core logic of the risk regulator to the structure of the database and the rules governing access control, creating a unified and traceable development environment. This approach is central to fulfilling the requirements of IEC 62304 Class C, which mandates detailed design documentation that is sufficient for correct implementation and allows for traceability from design to implementation and verification [[8](https://www.qualio.com/blog/iec-62304), [12](https://elsmar.com/elsmarqualityforum/threads/62304-class-c-in-practice.89372/)]. By using ALN as the single source of truth, developers can ensure that the system's behavior is precisely defined and that every component adheres to the formally verified safety model.

The primary ALN artifact, `neuromorph_regulator_v1.aln`, defines the core `risk_compute_formula` function ^1_1]]. This file is the definitive implementation of the risk calculation, containing the fixed weights, normalization semantics, and clamping logic that were subject to formal verification ^1_1]]. Its deterministic nature, combined with the provided `hash_stamp`, makes it the canonical reference against which all other implementations—including hardware-accelerated versions—must be validated ^1_1]]. The proposed extension, `RiskEnvelopeBqlf.aln`, builds upon this by defining the complete state vector of the BQLF as a typed dataset ^1_1]]. This artifact specifies the precise types, bounds, and semantic meanings of every field in the `RiskEnvelopeBqlf` record. It formalizes the mathematical contract that the rest of the system must honor, ensuring that neuromorphic telemetry, ledger slices, and risk indices are correctly aggregated into a single, coherent structure ^1_1]]. This dataset definition is a critical piece of the detailed design documentation required by IEC 62304, as it precisely describes the software's internal and external interfaces [[12](https://elsmar.com/elsmarqualityforum/threads/62304-class-c-in-practice.89372/)]. When the `RiskEnvelopeBqlf` dataset is included in the same namespace and the ALN text is re-hashed, a new, unique `hash_stamp` is generated, continuing the chain of immutable, auditable updates that link each version of the specification to its corresponding implementation and test evidence ^1_3]]1]].

Beyond the core logic, the ALN artifacts define the persistence and security layers necessary for a production system. The `neuromorph_regulator_db_v1.aln` artifact specifies the minimal database schema required to store the risk samples and user identities separately, a design choice aligned with privacy-preserving principles like those found in GDPR and HIPAA ^2_1]]2_2]]. This schema cleanly separates personally identifiable information (PII) in the `regulator_user` table from the analyzable risk data in the `regulator_state` table ^2_1]]. The tagging of fields with attributes like `[pii_id]` and `[audit_ts]` directly instructs the runtime environment on how to handle sensitive data, for example, by automatically applying encryption or logging restrictions, which is a direct mapping of policy to implementation ^2_2]]. The associated access control policy, `neuromorph_access_control.aln`, further hardens the system by defining roles and permissions that prevent unauthorized access to PII and ensure data minimization ^2_1]]2_2]]. The `regulator_service` role, for instance, is granted permission to read and write the `regulator_state` table but is explicitly denied access to the `user_id` column in `regulator_user`, forcing the service to operate on pseudonymized identifiers ^2_2]]. This granular, tag-based constraint system provides a strong, machine-checkable guardrail against common data leakage patterns, which is essential for maintaining compliance and protecting patient privacy ^2_2]]. The combination of the database schema and access control policy creates a robust security boundary around the BQLF data, ensuring its integrity and confidentiality.

The use of ALN artifacts facilitates a seamless integration with code generation and formal verification workflows. The deterministic nature of ALN files makes them ideal inputs for generating Rust structs, which can then be compiled with Rust's borrow checker to provide memory-safe and thread-safe code [[11](https://dl.acm.org/doi/abs/10.1145/3689374)]. Furthermore, the formal definitions within the ALN files, particularly the mathematical expressions in the `risk_compute_formula`, can be directly imported into theorem provers like Coq or Isabelle. This allows for the automatic generation of formal theories that contain the axioms and lemmas representing the system's design, significantly reducing the effort required to build the machine-checkable proofs of safety invariants [[11](https://dl.acm.org/doi/abs/10.1145/3689374)]. For example, the proof that the `risk_score` is bounded by 1.0 can be constructed by importing the formula for `raw` and the definitions of the normalization and clamping functions into Coq and then executing a series of algebraic simplifications and logical deductions. This tight coupling between the ALN specification and the formal verification toolchain is a hallmark of a mature development process for safety-critical systems. It ensures that the formal proofs are always based on the most current version of the specification, eliminating the possibility of discrepancies between what is documented, what is implemented, and what is proven. Ultimately, the ALN artifacts provide the bridge between the abstract world of formal methods and the concrete world of deployed software, enabling a development lifecycle that is both highly productive and rigorously compliant with regulatory standards.

## Implementation Blueprint: Host Runtime and Neuromorphic Hardware Integration

The implementation of the Bounded Quantitative Logic Frame (BQLF) system is architected around a clear division of labor between a deterministic host runtime and specialized neuromorphic hardware coprocessors like Loihi or Akida [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. This hybrid architecture leverages the strengths of each platform: the host CPU handles tasks requiring strict determinism, security, and complex logic, while the neuromorphic backend accelerates the computationally intensive, parallelizable aspects of the risk calculation [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. This design philosophy is critical for meeting the stringent requirements of IEC 62304 Class C, as it allows for the segregation of safety-critical logic from performance-critical workloads, a principle that enhances both reliability and efficiency. The host runtime acts as the orchestrator, fulfilling the responsibilities of cryptographic hashing, database I/O, final clamping, and policy evaluation, all of which must be performed deterministically to ensure the integrity of the BQLF [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The neuromorphic backend, in turn, is tasked with efficiently computing the linear part of the `risk_compute_formula`, providing a significant speedup without compromising the correctness of the overall algorithm.

The host runtime's responsibilities are extensive and centered on maintaining the system's safety and security guarantees. First and foremost, it is responsible for implementing the `risk_compute_formula` in its entirety, except for the linear readout stage, which is delegated to the neuromorphic backend [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. This begins with input validation, ensuring that all incoming parameters are within their expected ranges before any processing occurs. Following validation, the host computes the normalized feature vector `[depth_n, energyn, auet_n, csp_n]` according to the precise equations defined in the ALN specification [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. This involves performing floating-point arithmetic and clamping operations, which are executed deterministically on the CPU. Once the features are computed, they are converted into a format suitable for the Loihi hardware, typically involving the conversion of floating-point values into spike counts or firing rates that correspond to the numeric range of the neuromorphic neurons [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. After receiving the result from the Loihi controller, the host applies the final post-processing steps, including the clamping of the raw score and the multiplication by the `K_PSYCH` factor to compute the `sf_psych` value [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. Crucially, the SHA3-512 hashing of the input triple `(user_id, bio_key, t_unix)` to compute the `h_mod` is also performed on the host, ensuring that this cryptographic operation is not delegated to the neuromorphic substrate, which lacks the necessary guarantees of determinism and security [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. Finally, the host constructs the `RiskEnvelopeBqlf` struct, populates it with all the necessary telemetry and ledger data, and writes the final, validated BQLF record to the database under the appropriate security context [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)].

The neuromorphic backend, configured as either a `LoihiBackend` or `AkidaBackend`, is responsible for a specific, well-defined task: implementing the weighted sum of the four input features [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. On the host side, a small Spiking Neural Network (SNN) graph is designed to mirror the mathematical structure of the `risk_compute_formula`. This graph consists of four input neuron populations, one for each feature, and a single readout population [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The synaptic weights connecting the input populations to the readout population are initialized with the fixed coefficients from the ALN file (`W_DEPTH`, `W_ENERGY`, `W_AUET`, `W_CSP`) [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The host sends the pre-computed normalized features to the Loihi hardware as spike trains. The Loihi chip then performs the weighted sum in parallel, with each input neuron contributing to the firing rate of the readout neuron based on its assigned weight. The output of the readout neuron is a value proportional to the `raw` score. This approach is grounded in rigorous control theory for neuromorphic systems. Research papers demonstrate systematic design methods for creating spiking controllers that stabilize linear time-invariant plants, using emulation-based approaches to ensure that the closed-loop system remains stable and bounded [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. By treating the Loihi network as a physical realization of the linear part of the risk equation, this methodology provides a strong theoretical foundation for its correct operation.

To ensure the reliability of this hybrid architecture, a rigorous conformance testing protocol is essential. This protocol is built around a "golden CPU reference implementation," which is simply the standard `risk_compute_formula` running entirely on the host CPU without any neuromorphic acceleration [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. Every invocation of the accelerated function on Loihi must be paired with a call to the reference implementation, and the outputs must be compared under predefined numerical tolerances [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. These test vectors, along with the acceptance criteria, are themselves stored as ALN datasets, creating a closed loop of verification where the validation assets are managed alongside the source code and specifications [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The prerequisites for deploying this system include a fully documented software lifecycle aligned with IEC 62304, a supported SDK for the Loihi hardware, a host environment capable of performing the required crypto (e.g., TLS 1.3, AES-256-GCM), and segregated network segments for the lab environment to isolate it from clinical workflows [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The TSN timing profile specified in the vnode shard, with a latency window of 100–500 microseconds, further constrains the neuromorphic backend's real-time performance, ensuring that the system meets its deadlines in a clinical setting [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. This detailed blueprint ensures that the integration of cutting-edge neuromorphic technology is done in a controlled, verifiable, and safe manner, preserving the integrity of the BQLF and its underlying safety invariants.

## Database Schema and Access Control for Auditable Persistence

For a system operating in a regulated medical environment, the persistence layer must be designed with an unwavering focus on security, integrity, and auditability. The `neuromorph_regulator_db_v1.aln` artifact provides a minimal yet robust database schema that addresses these requirements head-on, separating identity from data, enforcing encryption, and explicitly marking fields for special handling ^2_1]]2_2]]. This schema is not just a technical specification but a critical component of the overall safety and compliance framework, directly supporting the principles of IEC 62304 and ISO 14971. By meticulously controlling how data is stored and accessed, the database serves as the ground truth for the Bounded Quantitative Logic Frame (BQLF), ensuring that historical records are accurate, tamper-evident, and protected against unauthorized disclosure.

The core design principle of the proposed database schema is the strict separation of Personally Identifiable Information (PII) from analytical data. The `regulator_user` table stores raw identifiers like `user_id` and the timestamp of creation, tagged with `[pii_id]` and `[audit_ts]` respectively ^2_1]]2_2]]. This table is the authoritative source for linking a risk sample back to a specific individual, but its contents are handled with the highest level of security. In contrast, the `regulator_state` table contains the actual risk samples—the `risk_score`, `ed_percent`, `sf_psych`, and the `h_mod` hash—as well as a foreign key `user_id_fk` linking back to the `regulator_user` table ^2_1]]. This design ensures that any analytics workload that does not require direct human interaction can operate exclusively on pseudonymized data, thereby minimizing the exposure of sensitive PHI and aligning with the data minimization guidance of regulations like GDPR and HIPAA ^2_2]]. The `h_mod` field plays a crucial role in this separation; it is a deterministic hash of the input data, allowing for the detection of duplicates or tampering without revealing the raw inputs ^2_1]]. This separation of concerns is fundamental to building a system that is both powerful for analysis and respectful of patient privacy.

Access control is enforced through a granular, role-based policy defined in the `neuromorph_access_control.aln` file, which binds directly to the database artifact ^2_1]]2_2]]. Three distinct roles are defined to model different classes of users and their required privileges. The `regulator_service` role represents the main application logic and is granted broad permissions to read and write to the `regulator_state` table, allowing it to process new inputs and store new risk samples ^2_2]]. Critically, this role is denied access to the `user_id` column in the `regulator_user` table, forcing it to operate solely on the pseudonymized `pseudonym_id` or `user_id_fk` received from a secured service boundary ^2_2]]. This prevents the service logic from ever having access to raw PII, a key defense-in-depth measure. The `auditor_strict` role is designed for authorized personnel who need to perform audits or investigations that may require re-identification. This role is granted read access to all fields in both tables but is subject to stringent requirements, such as multi-factor authentication (MFA) and access only from a designated, secured network zone ("clin-secured"), providing a high-assurance channel for justified re-identification ^2_2]]. Finally, the `analytics_batch` role is scoped for aggregate analysis. It can read only the three numeric risk metrics from `regulator_state` but is explicitly denied access to the `h_mod` field and any user-related fields, preventing it from joining risk data with external datasets using quasi-identifiers ^2_2]].

A crucial enhancement to this access control model is the `pii_export_guard` constraint, which adds an additional layer of protection for PII ^2_2]]. This constraint encodes a policy that any field tagged with `[pii_id]` can only be exported to a predefined, hardened destination, such as an on-premise vault ^2_2]]. This policy is machine-checkable and prevents accidental data leakage to less secure environments. It codifies the "vaulted PHI" pattern seen in larger Cybercore CEM designs, where raw personal data is locked down, and only processed, non-identifying metrics are allowed into research or analytics zones ^2_2]]. The combination of a clean database schema, granular role-based access control, and tag-based export constraints creates a comprehensive security framework. This framework is demonstrably aligned with the requirements of IEC 62304 and ISO 14971. It provides clear, auditable controls over data access, minimizes the exposure of sensitive information, and establishes a robust defense against data breaches and misuse. The explicit tagging of fields for encryption and auditing also streamlines the process of meeting GDPR and HIPAA logging and protection requirements, as the runtime can automatically apply the correct policies based on these tags ^2_2]]. Overall, the database and access control design provides the necessary foundation for storing the BQLF in a manner that is secure, compliant, and ready for inspection by external auditors and regulatory reviewers.

## End-to-End Traceability: From BQLF Invariants to Regulatory Substantiation

The ultimate goal of developing a safety-critical system is to provide compelling evidence to external auditors and regulatory bodies that the software is safe and effective. The Bounded Quantitative Logic Frame (BQLF) and its associated formal verification model provide the ideal vehicle for achieving this, as they establish a direct and unbroken chain of traceability from high-level regulatory requirements down to machine-checkable proofs of correctness. This traceability is not an afterthought but is woven into the fabric of the development process, satisfying the explicit mandates of standards like IEC 62304 and ISO 14971 for linking risk controls to their implementation and verification [[5](https://www.greenlight.guru/blog/iso-14971-risk-management), [6](https://www.achievexl.com/blog/risk-management-file-structure-made-easy)]. An auditor reviewing the submission package would find a coherent and logically sound argument for the system's safety, built upon a foundation of mathematical certainty.

This traceability chain begins at the highest level with the **Risk Management File (RMF)**, which documents the application's safety classification and risk management plan [[5](https://www.greenlight.guru/blog/iso-14971-risk-management)]. The RMF identifies potential hazards, such as a "failure to accurately calculate the risk score leading to incorrect treatment decisions." For each hazard, the RMF links to a corresponding **Risk Control Measure**, which in this case is the `risk_compute_formula` and its associated BQLF update function. The RMF then points to the **Verification Evidence** that demonstrates the effectiveness of this control. Here, the auditor would find a reference to the formal proofs of the BQLF's safety invariants. This linkage is made explicit through the `hash_stamp` of the relevant ALN artifact. For example, the RMF entry for the hazard might state: "Control: `risk_compute_formula` as defined in `neuromorph_regulator_v1.aln` (hash: ...). Verification: Formal proof of boundedness, monotonicity, and 1-Lipschitz continuity contained in Coq/Isabelle theories linked to the same hash." This creates an auditable trail from the abstract hazard to the concrete proof, satisfying the completeness and traceability requirements of ISO 14971 Clause 4.5 [[6](https://www.achievexl.com/blog/risk-management-file-structure-made-easy)].

The next layer of traceability connects the high-level requirements to the detailed design. IEC 62304 Class C requires that the detailed design documentation be sufficient for correct implementation [[12](https://elsmar.com/elsmarqualityforum/threads/62304-class-c-in-practice.89372/)]. The `RiskEnvelopeBqlf.aln` dataset serves this purpose perfectly. It is a formal, executable specification of the system's internal state. The auditor can inspect this ALN file to see the precise types, bounds, and relationships of every field in the BQLF. This document directly answers questions about how the software is structured internally. Furthermore, the detailed design must trace back to the high-level requirements. The `RiskEnvelopeBqlf` definition is a direct manifestation of the requirement that the system must aggregate and monitor a set of bounded risk metrics. The `neuromorph_regulator_db_v1.aln` and `neuromorph_access_control.aln` artifacts complete this picture by showing how the design is realized in persistent storage and secured with access controls, demonstrating a holistic approach to design that considers not just functionality but also security and data management ^2_1]]2_2]].

The final link in the chain connects the design to the implementation and testing. The formal proofs generated in Coq or Isabelle are the most critical piece of evidence here. They are not just documents; they are machine-verifiable artifacts that confirm the software's behavior. The auditor can examine the proofs to see that they correctly import the definitions from the ALN artifacts and apply logical reasoning to conclude that the safety invariants hold. This is a much stronger form of verification than traditional testing, as it provides exhaustive coverage. The conformance tests that compare the CPU reference implementation to the Loihi-accelerated version are also part of this chain. These tests, stored as ALN datasets, demonstrate that the hardware-accelerated implementation correctly realizes the behavior specified in the formal design [[14](https://css.paperplaza.net/conferences/conferences/CDC25/program/CDC25_ContentListWeb_1.html)]. The entire development history, from initial requirements to final test results, is anchored by the ALN artifacts and their associated hash stamps, creating a complete and immutable audit trail that can be followed from start to finish.

In summary, the submission package for regulatory review should be organized around this traceability chain. It would begin with the high-level Risk Management File, proceed to the detailed design specifications (the ALN artifacts), and culminate in the machine-checkable proofs and test evidence. This structure provides a compelling narrative of the system's development process: a systematic, risk-based approach that leverages formal methods to achieve a high degree of assurance. By presenting the BQLF as the central hub of this ecosystem, the developer demonstrates a sophisticated understanding of modern software engineering practices for safety-critical systems. This approach not only satisfies the letter of the regulatory requirements but also embodies the spirit of proactive risk management, providing auditors and regulators with the confidence that the system is not just compliant, but truly safe.